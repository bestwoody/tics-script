# 易观 OLAP 参赛：自研发方案

## 开发进度
```
***** 持久化格式设计与实现：文件格式，目录组织
***** 并发策略设计与实现：多文件、文件内并发，达到可用效率
****- 谓词下推：时间主键已用于剪枝、下推，事件属性表达式未下推
***** 漏斗计算：已验证正确
----- 集群功能：RPC，分桶导入数据

***** 单机命令行工具集
----- 集群工具集：导数据，分发查询，汇总结果
****- 性能优化：扫表尚有优化空间
```

## 用况与架构
* 用况一：开发、测试
    - 在命令行启动程序
    - 程序操作本地文件
        - 将原始数据导入
            - 解析原始数据分区文件
            - 排序
            - 写入排序后数据分区文件
            - 写入索引文件
        - 验证导入数据的正确性
            - Dump 索引文件
            - Dump 数据
* 用况二：单机跑数据
    - 在命令行启动程序
    - 程序在本地已导入数据中扫描数据，完成需求
* 用况三：集群跑数据
    - 在命令行启动客户端程序
    - 客户端程序发送 rpc 请求至多个服务端
    - 多个服务端各自扫描数据，计算出小结果集
    - 客户端汇总结果，完成需求

## 目录布局
* 原始数据
    - `[Origin]`
        - `20170101`            - 文本数据，每天一个
        - `20170102`
        - `...`
* 导入后数据
    - `[DB]`
        - `20170101.data`       - 数据文件
        - `20170102.idx`        - 数据文件的索引文件
        - `...`

## 数据逻辑
* 将所有数据全局排序
    - 原始数据已经按天分割，每文件内部排好序即可
* 将排好序的数据按 hash(uid) 进行分区，分区数量=集群机器数
    - 往各服务器导入对应的用户数据，保持有序
* 查询时各服务器按条件独自计算结果
    - 由于按用户分区，单台服务器包含该用户的所有数据，可以直接出结果
    - 结果数据：`map <到达路径深度, 用户个数>`
    - 多台的结果可以直接按 Key 叠加，合并后格式同上

## 数据格式
```
[DB]
    Partition * N
        DataFile * N
            Block * N              - align by 512
                MagicFlag          - uint16
                CompressType       - uint16, def: snappy
                OriginSize         - uint64, only use in cxx version
                RowCount           - uint32, def: 8192
                Row * N                                         ---  ---
                    Timestamp      - uint32                       |    | compressed
                    UserId         - uint32                       |    |
                    EventId        - uint16                       |    |
                    EventPropsLen  - uint16                       |    |
                    EventProps     - []byte, json                 |  ---
                BlockChecksum      - origin data crc32 <-- crc32 -|
        IndexFile
            EntryCount             - uint32
            IndexEntry * N                                      ---
                Timestamp          - uint32                       |
                BlockOffset        - uint32                       |
                BlockSize          - uint32                       |
            IndexEntry             - the end of the N-st Entry    |
            IndexChecksum          - uint32            <-- crc32 -|

crc32: use castagnoli polynomials, same as sse4.
```

## 测试状况
* 当前小样数据
    - 原始数据
        - 5G 压缩数据，展开为 30G
        - 包含 2 个月数据，每天一个文件
            - 每天的文件为 500M 一个
            - 1 千万行
        - 总约 6 亿行
    - 使用 snappy 压缩参数导入后为 4.9G
        - 体积
            - 压缩后每天文件缩减为 80M
            - 索引大小每数据文件 9K
        - 导入
            - 导入过程在 32 核服务器上并发的参数为 16 时最佳
            - 导入耗时 6 分半
                - 占用 150% CPU
                - 吞吐 85~100MB/s
            - 单文件较大，开高并发容易引起 OOM
            - 有较大提升余地，暂时没必要继续优化
            - 使用 8K 的粗索引粒度
                - 测过多轮，该值为良好实践
                - CH 的推荐值
            - 512 字节的 Block 对齐
                - 尚未测试不同对齐的影响
        - 扫描
            - 点扫
                - 有时间列索引
                - 读放大不超过 8K 行
            - 大扫描
                - 全数据扫描一次需要 30 秒
                - 扫描时 16~32 并发均为良好参数
                    - 占用 300%～400% 左右 CPU
                    - 吞吐 110MB/s+
                - 核心优化点，与需求密切相关
                    - 当前性能 = 90% 单盘（160MB带宽）吞吐上限
                    - 在更快的盘上表现如何，尚未测定
                        - 用 C++/Rust 会更有优势，GO 的 GC 问题严重
                            - 未测，未仔细定位
                    - 仍有提升空间，优化点：
                        - 并发调度策略
                        - 流水线：主要在`读盘、解码解压、计算`之间并发
                        - 压缩算法选择
                        - 缓存策略
            - 含谓词扫描
                - 原始数据每行中的 Event 可能有属性，json 格式
                - 当前导入后仍旧为 json
                    - 需要进一步处理，例如改写为 kv 集合
                - 谓词过滤方案
                    - 方案一：在该项目中实现简单的表达式解析器
                    - 方案二：收到需求后写入源代码，现场编译和部署，再跑数据出结果
                    - 方案三：以 Rpc 方式接入 Spark，复杂需求由 Spark SQL 读数据后计算
